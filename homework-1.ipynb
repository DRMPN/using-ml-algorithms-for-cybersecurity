{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":117574,"databundleVersionId":14113605,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:38:34.703049Z","iopub.execute_input":"2025-11-12T02:38:34.703287Z","iopub.status.idle":"2025-11-12T02:38:36.993481Z","shell.execute_reply.started":"2025-11-12T02:38:34.703266Z","shell.execute_reply":"2025-11-12T02:38:36.992522Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/whos-talking-classify-the-app-by-its-packets/sample_submission.csv\n/kaggle/input/whos-talking-classify-the-app-by-its-packets/train.csv\n/kaggle/input/whos-talking-classify-the-app-by-its-packets/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom lightgbm import LGBMClassifier\nfrom scipy.stats import randint, uniform\n\n# --- Configuration ---\nQUICK_TEST = False \nN_ITER = 50 if not QUICK_TEST else 5\nCV_FOLDS = 5 if not QUICK_TEST else 2\nN_JOBS_FOR_SEARCH = 4 # Keep low to avoid OOM\nRANDOM_STATE = 42\n\n# Define feature columns\nFEATURE_COLS = [f'tcp_len_{i}' for i in range(1, 31)]\nTARGET_COL = \"app_service\"\nKAGGLE_INPUT_PATH = '/kaggle/input/whos-talking-classify-the-app-by-its-packets/'\n\n# --- 1. Feature Engineering Functions ---\n\ndef process_flow(row):\n    \"\"\"\n    Applies the preprocessing logic from the competition description.\n    Merges consecutive, same-direction, large (>1200) packets.\n    \"\"\"\n    new_flow = []\n    flow_data = row.values \n    i = 0\n    \n    while i < 30:\n        packet = flow_data[i]\n        if packet == 0:\n            break\n            \n        sign = np.sign(packet)\n        size = np.abs(packet)\n        \n        if size > 1200:\n            current_sum = 0\n            j = i\n            while j < 30:\n                next_packet = flow_data[j]\n                next_sign = np.sign(next_packet)\n                next_size = np.abs(next_packet)\n                \n                if next_sign == sign and next_size > 1200:\n                    current_sum += next_size\n                    j += 1\n                else:\n                    break\n            \n            new_flow.append(sign * current_sum)\n            i = j\n        \n        else:\n            new_flow.append(packet)\n            i += 1\n            \n    padded_flow = new_flow + [0] * (30 - len(new_flow))\n    return padded_flow[:30]\n\n\ndef extract_statistical_features(df):\n    \"\"\"\n    Inspired by the malware notebook, this function extracts\n    high-level statistical features from a flow (row).\n    \"\"\"\n    print(\"Extracting statistical features...\")\n    # df is the (n_samples, 30) dataframe of packet lengths\n    features = pd.DataFrame(index=df.index)\n    \n    # Replace 0s (padding) with NaN to ignore them in stats\n    df_nan = df.replace(0, np.nan)\n    \n    # --- Overall Stats ---\n    features['num_packets'] = df_nan.notna().sum(axis=1)\n    features['total_bytes'] = df_nan.abs().sum(axis=1)\n    features['avg_packet_size'] = df_nan.abs().mean(axis=1)\n    features['std_packet_size'] = df_nan.abs().std(axis=1)\n    \n    # --- Client Stats (positive values) ---\n    client_df = df_nan[df_nan > 0]\n    features['num_client_packets'] = client_df.notna().sum(axis=1)\n    features['total_client_bytes'] = client_df.sum(axis=1)\n    features['avg_client_packet_size'] = client_df.mean(axis=1)\n    features['std_client_packet_size'] = client_df.std(axis=1)\n    features['max_client_packet'] = client_df.max(axis=1)\n    features['min_client_packet'] = client_df.min(axis=1)\n\n    # --- Server Stats (negative values) ---\n    server_df = df_nan[df_nan < 0]\n    features['num_server_packets'] = server_df.notna().sum(axis=1)\n    # .abs() is important for sum/mean\n    features['total_server_bytes'] = server_df.abs().sum(axis=1)\n    features['avg_server_packet_size'] = server_df.abs().mean(axis=1)\n    features['std_server_packet_size'] = server_df.abs().std(axis=1)\n    features['max_server_packet'] = server_df.abs().max(axis=1) # Max *size*\n    features['min_server_packet'] = server_df.abs().min(axis=1) # Min *size*\n\n    # --- Ratio Stats ---\n    # Handle division by zero\n    features['ratio_client_server_packets'] = features['num_client_packets'] / (features['num_server_packets'] + 1e-6)\n    features['ratio_client_server_bytes'] = features['total_client_bytes'] / (features['total_server_bytes'] + 1e-6)\n    \n    # --- Simple Sequence Stats ---\n    features['first_packet'] = df.iloc[:, 0]\n    features['second_packet'] = df.iloc[:, 1]\n    \n    # Fill all NaNs from stats (e.g., std of 1 packet, avg of 0 packets) with 0\n    return features.fillna(0).astype('float32')\n\n# --- 2. Load and Preprocess Training Data ---\n\nprint(\"Script started.\")\nprint(\"Loading training data...\")\ntry:\n    train_df = pd.read_csv(f'{KAGGLE_INPUT_PATH}train.csv', usecols=[TARGET_COL] + FEATURE_COLS)\nexcept FileNotFoundError:\n    print(\"Error: train.csv not found.\")\n    raise\n\nif QUICK_TEST:\n    print(\"Running in QUICK_TEST mode. Subsampling training data.\")\n    train_df = train_df.sample(n=50000, random_state=RANDOM_STATE)\n\n# Encode the target variable\nle = LabelEncoder()\ny = le.fit_transform(train_df[TARGET_COL].astype(str))\nnum_classes = len(le.classes_)\n\n# --- 3. Combine Feature Sets ---\n\n# Feature Set 1: Raw Features\nprint(\"1. Processing Raw Features...\")\nX_raw = train_df[FEATURE_COLS].fillna(0).astype('float32')\n# Rename columns to avoid collision\nX_raw.columns = [f'raw_{i}' for i in range(1, 31)]\n\n# Feature Set 2: Statistical Features\nprint(\"2. Processing Statistical Features...\")\nX_stats = extract_statistical_features(train_df[FEATURE_COLS])\n# Scale statistical features\nscaler = StandardScaler()\nX_stats = pd.DataFrame(scaler.fit_transform(X_stats), columns=X_stats.columns, index=X_stats.index).astype('float32')\n\n# Feature Set 3: Merged Packet Features\nprint(\"3. Processing Merged Packet Features...\")\nX_merged = train_df[FEATURE_COLS].fillna(0).apply(process_flow, axis=1, result_type='expand')\nX_merged.columns = [f'merged_{i}' for i in range(1, 31)]\nX_merged = X_merged.astype('float32')\n\n# Clean up original training data to save RAM\nprint(\"Cleaning up original train_df...\")\ndel train_df\ngc.collect()\n\n# Combine all feature sets\nprint(\"Combining all feature sets...\")\nX = pd.concat([X_raw, X_stats, X_merged], axis=1)\n\nprint(f\"Total features created: {X.shape[1]}\")\nprint(f\"Training data shape: {X.shape}, Memory usage: {X.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n\n# Clean up intermediate dataframes\ndel X_raw, X_stats, X_merged\ngc.collect()\n\n# --- 4. Hyperparameter Optimization ---\n\nprint(\"Setting up RandomizedSearchCV...\")\nlgbm = LGBMClassifier(\n    objective=\"multiclass\",\n    num_class=num_classes,\n    metric=\"multi_logloss\",\n    random_state=RANDOM_STATE,\n    n_jobs=-1  # Use all 77 cores for training *each* model\n)\n\nparam_dist = {\n    'n_estimators': randint(300, 1200),\n    'learning_rate': uniform(0.01, 0.05),\n    'num_leaves': randint(31, 61),\n    'max_depth': [-1, 20, 30],\n    'feature_fraction': uniform(0.5, 0.4), # 0.5 to 0.9\n    'bagging_fraction': uniform(0.5, 0.4), # 0.5 to 0.9\n    'bagging_freq': randint(1, 7),\n    'min_child_samples': randint(20, 50)\n}\n\nrandom_search = RandomizedSearchCV(\n    estimator=lgbm,\n    param_distributions=param_dist,\n    n_iter=N_ITER,        \n    cv=CV_FOLDS,          \n    scoring='accuracy',   \n    n_jobs=N_JOBS_FOR_SEARCH, # <-- CRITICAL: Low job count for search\n    random_state=RANDOM_STATE,\n    verbose=2\n)\n\nprint(f\"Starting model training with {N_JOBS_FOR_SEARCH} parallel jobs.\")\nrandom_search.fit(X, y)\n\nprint(f\"Best parameters found: {random_search.best_params_}\")\nprint(f\"Best cross-validation accuracy: {random_search.best_score_:.4f}\")\n\nbest_model = random_search.best_estimator_\n\nprint(\"Cleaning up training data...\")\ndel X, y, random_search, lgbm\ngc.collect()\n\n# --- 5. Load and Process Test Data ---\n\nprint(\"Loading test data...\")\ntry:\n    test_df = pd.read_csv(f'{KAGGLE_INPUT_PATH}test.csv')\nexcept FileNotFoundError:\n    print(\"Error: test.csv not found.\")\n    raise\n\ntest_ids = test_df['id']\n\nprint(\"Applying same feature engineering to test data...\")\n# Feature Set 1: Raw Features\nprint(\"1. Processing Raw Features (Test)...\")\nX_raw_test = test_df[FEATURE_COLS].fillna(0).astype('float32')\nX_raw_test.columns = [f'raw_{i}' for i in range(1, 31)]\n\n# Feature Set 2: Statistical Features\nprint(\"2. Processing Statistical Features (Test)...\")\nX_stats_test = extract_statistical_features(test_df[FEATURE_COLS])\n# IMPORTANT: Use the *same scaler* fitted on the training data\nX_stats_test = pd.DataFrame(scaler.transform(X_stats_test), columns=X_stats_test.columns, index=X_stats_test.index).astype('float32')\n\n# Feature Set 3: Merged Packet Features\nprint(\"3. Processing Merged Packet Features (Test)...\")\nX_merged_test = test_df[FEATURE_COLS].fillna(0).apply(process_flow, axis=1, result_type='expand')\nX_merged_test.columns = [f'merged_{i}' for i in range(1, 31)]\nX_merged_test = X_merged_test.astype('float32')\n\n# Clean up original test data\ndel test_df\ngc.collect()\n\n# Combine all feature sets for test data\nprint(\"Combining all feature sets (Test)...\")\nX_test = pd.concat([X_raw_test, X_stats_test, X_merged_test], axis=1)\n\nprint(f\"Test data shape: {X_test.shape}\")\ndel X_raw_test, X_stats_test, X_merged_test\ngc.collect()\n\n# --- 6. Generate Predictions ---\n\nprint(\"Generating predictions on the test set...\")\npredictions_encoded = best_model.predict(X_test)\npredictions = le.inverse_transform(predictions_encoded)\n\ndel X_test, best_model, predictions_encoded\ngc.collect()\n\n# --- 7. Create Submission File ---\n\nprint(\"Creating submission file...\")\nsubmission_df = pd.DataFrame({'id': test_ids, TARGET_COL: predictions})\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"Script finished successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:38:36.995057Z","iopub.execute_input":"2025-11-12T02:38:36.995530Z","iopub.status.idle":"2025-11-12T02:55:53.175815Z","shell.execute_reply.started":"2025-11-12T02:38:36.995506Z","shell.execute_reply":"2025-11-12T02:55:53.173451Z"}},"outputs":[{"name":"stdout","text":"Script started.\nLoading training data...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_39/4174346589.py:119: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n  train_df = pd.read_csv(f'{KAGGLE_INPUT_PATH}train.csv', usecols=[TARGET_COL] + FEATURE_COLS)\n","output_type":"stream"},{"name":"stdout","text":"1. Processing Raw Features...\n2. Processing Statistical Features...\nExtracting statistical features...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n","output_type":"stream"},{"name":"stdout","text":"3. Processing Merged Packet Features...\nCleaning up original train_df...\nCombining all feature sets...\nTotal features created: 80\nTraining data shape: (8248546, 80), Memory usage: 2.46 GB\nSetting up RandomizedSearchCV...\nStarting model training with 4 parallel jobs.\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_39/4174346589.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting model training with {N_JOBS_FOR_SEARCH} parallel jobs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best parameters found: {random_search.best_params_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1782\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m             \u001b[0merror_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# be returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}\nDetailed tracebacks of the workers should have been printed to stderr in the executor process if faulthandler was not disabled."],"ename":"TerminatedWorkerError","evalue":"A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}\nDetailed tracebacks of the workers should have been printed to stderr in the executor process if faulthandler was not disabled.","output_type":"error"}],"execution_count":2}]}